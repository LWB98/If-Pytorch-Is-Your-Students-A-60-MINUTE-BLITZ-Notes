## 3.Autograd运行原理

使用参数`requires_grad=True`来对张量a,b进行跟踪，从而了解它们的每个操作


```python
import torch

a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)
```


```python
a
```


    tensor([2., 3.], requires_grad=True)


```python
b
```


    tensor([6., 4.], requires_grad=True)

设函数Q(a,b)，我们来计算一下它的偏导


```python
Q = 3*a**3 - b**2
Q
```




    tensor([-12.,  65.], grad_fn=<SubBackward0>)



假设a,b是NN的参数，Q是误差函数，在NN的训练中，我们需要计算它们的偏导，即
$$
Q\left( a,b \right) =3a^3-b^2
\\
Q'_a=9a
\\
Q'_b=2b
$$

当我们在Q上调用`.backward()`时，autograd会计算这些梯度并将它们存储在相应张量的`.grad`属性之中


```python
external_grad = torch.tensor([1., 1.])
Q.backward(gradient=external_grad)
# check if collected gradients are correct
print(9*a**2 == a.grad)
print(-2*b == b.grad)
```

    tensor([True, True])
    tensor([True, True])

