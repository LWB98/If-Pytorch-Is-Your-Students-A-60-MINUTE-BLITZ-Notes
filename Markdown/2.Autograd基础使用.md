源文档：[DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)

环境：Win 10 + JupyterLab 3.3.2 +Conda Environment(dl-env.yaml)

## 2.Autograd基础使用


```python
# import torch, torchvision
# model = torchvision.models.resnet18(pretrained=True)
# data = torch.rand(1, 3, 64, 64)
# labels = torch.rand(1, 1000)
```

**bug1:模型下载失败**

因为网络问题所以运行以下代码，使用离线下载好的模型。
具体方法参考https://blog.csdn.net/haohulala/article/details/107598859


```python
import torch, torchvision

model = torchvision.models.resnet18(pretrained=False)
load = torch.load("./model/resnet18-5c106cde.pth")
model.load_state_dict(load)
```


    <All keys matched successfully>




```python
#创建初始数据
data = torch.rand(1, 3, 64, 64)
labels = torch.rand(1, 1000)
```


```python
prediction = model(data) # forward pass
```

这里的`prediction`是一个很大的数据，此处没有理解用意

Autograd计算每个模型的参数的梯度并将其存储在`.grad`属性中


```python
loss = (prediction - labels).sum()
loss.backward() # backward pass
```

加载优化器


```python
optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
```

进行梯度下降


```python
optim.step() #gradient descent
```

文彬